---
title: "Measuring Meaning (pt. 1/3)"
description: Turning words into numbers
date: 2024-09-09
format: html
categories: [Semantic quantification]
image: The_stuff_of_thought_7ddbf523-f23d-421c-8765-578850f389b7.png
editor: visual
---

![](The_stuff_of_thought_7ddbf523-f23d-421c-8765-578850f389b7.png){fig-align="center"}

One of the key innovations that has contributed to the development of LLMs is the ability to convert words into embedded vectors, or vectors of word embeddings. This is because analysing text (formatted as strings) is challenging. After simply counting the relative frequencies of words, what else is there to quantify?

# N-grams

N-grams offered a way to capture some local context by focusing on combinations of words, but they struggle with longer-range dependencies. Simply increasing the size of your n-grams to capture more context leads to overly complex, dense representations that are inefficient and still fail to fully capture meaning over longer distances[^1]. 

# Transformers

It’s not enough to simply count words or look at combinations of words. The question remains: where do you draw the boundaries between letters, words, sentences, and contexts? The answer comes in the form of the **Transformer architecture**[^2], which preserves context and relevance by modeling the **relative importance** of different words through **self-attention**.

In this process, words are converted into **numerical representations** called **embeddings**, which capture their relationships to other words across multiple dimensions. These embeddings are adjusted based on the attention mechanism, which weighs how much focus each word should receive relative to others. Importantly, I say "words," but in practice, the model operates on **tokens**—units of analysis that may loosely correspond to words but can also include spaces, parts of words, punctuation, or combinations of these. Over time, as the model grows in size, it learns to assign precise **coordinates** (or embeddings) to each token, mapping them in a way that reflects their contextual importance and meaning.

In practice, the use of word embeddings offers a semantically rich quantification of language. The extent to which this is the case is demonstrated in the ability of LLMs to capture nuances like synonyms, antonyms, or contextual similarities.

In essence, by converting language into embeddings, we're compressing its meaning into a lower-dimensional space. While this might feel like 'flattening' language, there's an optimistic view: embeddings translate one form of language into another. This compression doesn’t strip away meaning but instead encodes it in ways that align with our linguistic and cognitive frameworks, allowing models to capture rich semantic relationships and potentially offer us new ones. 


[^1]: "Attention Is All You Need" - [Link](https://arxiv.org/abs/1706.03762)

[^2]: You may also have heard of N-grams due to their association with one [Robert L. Mercer](https://scholar.google.com/citations?user=9VCF3hQAAAAJ&hl=en), famous for associations with Cambridge Analytica, Brexit campaign, Breitbart News, and for his stint at elite Hedge Fund Renaissance Technologies. Who says NLP doesn't pay?
