---
title: "Measuring Meaning (1/3)"
description: Turning words into numbers
date: 2024-09-06
format: html
categories: [Semantic quantification]
image: In_essence_by_converting_language_into_embeddings_were_2dd21abf-b169-402c-b1bc-0472cbce47c9.png
editor: visual
---

![](In_essence_by_converting_language_into_embeddings_were_2dd21abf-b169-402c-b1bc-0472cbce47c9.png){fig-align="center"}

One of the key innovations that has contributed to the development of LLMs is the ability to convert words into embedded vectors, or vectors of word embeddings. This is because analysing text (formatted as strings) is challenging. After simply counting the relative frequencies of words, what else is there to quantify?

# N-grams

N-grams offered a way to capture some local context by focusing on combinations of words, but they struggle with longer-range dependencies. Simply increasing the size of your n-grams to capture more context leads to overly complex, dense representations that are inefficient and still fail to fully capture meaning over longer distances. That doesn't mean they don't have practical significance though; N-grams appear to have been the focus of one [Robert L. Mercer](https://scholar.google.com/citations?user=9VCF3hQAAAAJ&hl=en), famous for associations with Cambridge Analytica, Brexit campaign, Breitbart News, and for his stint at elite Hedge Fund Renaissance Technologies. Who says NLP doesn't pay?

# Transformers

It’s not enough to simply count words or look at combinations of words. The question remains: where do you draw the boundaries between letters, words, sentences, and contexts? The answer comes in the form of the **Transformer architecture**[^1], which preserves context and relevance by modeling the **relative importance** of different words through **self-attention**.

[^1]: "Attention Is All You Need" - [Link](https://arxiv.org/abs/1706.03762)

In this process, words are converted into **numerical representations** called **embeddings**, which capture their relationships to other words across multiple dimensions. These embeddings are adjusted based on the attention mechanism, which weighs how much focus each word should receive relative to others. Importantly, I say "words," but in practice, the model operates on **tokens**—units of analysis that may loosely correspond to words but can also include spaces, parts of words, punctuation, or combinations of these. Over time, as the model grows in size, it learns to assign precise **coordinates** (or embeddings) to each token, mapping them in a way that reflects their contextual importance and meaning.

In practice, the use of word embeddings offers a semantically rich quantification of language[^2]. The extent to which this is the case is demonstrated in the ability of LLMs to capture nuances like synonyms, antonyms, or contextual similarities.

[^2]: A reshaping, compression and reforming, emphasis intentional.

In essence, by converting language into embeddings, we're compressing its meaning into a lower-dimensional space. While this might feel like 'flattening' language, there's an optimistic view: embeddings translate one form of language into another. This compression doesn’t strip away meaning but instead encodes it in ways that align with our linguistic and cognitive frameworks, allowing models to capture rich semantic relationships.
